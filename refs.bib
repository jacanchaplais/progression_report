@article{gallicchio_2010,
    title={Seeing in Color: Jet Superstructure},
    volume={105},
    ISSN={1079-7114},
    url={http://dx.doi.org/10.1103/PhysRevLett.105.022001},
    DOI={10.1103/physrevlett.105.022001},
    number={2},
    journal={Physical Review Letters},
    publisher={American Physical Society (APS)},
    author={Gallicchio, Jason and Schwartz, Matthew D.},
    year={2010},
    month={Jul}
}

@misc{battaglia2018relational,
    title={Relational inductive biases, deep learning, and graph networks}, 
    author={Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andrew Ballard and Justin Gilmer and George Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
    year={2018},
    eprint={1806.01261},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{hyperopt,
    author = {Bergstra, J. and Yamins, D. and Cox, D. D.},
    title = {Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures},
    year = {2013},
    publisher = {JMLR.org},
    abstract = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method's full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned.In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included. A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric. Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures.},
    booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
    pages = {I–115–I–123},
    location = {Atlanta, GA, USA},
    series = {ICML'13}
}

@misc{asha,
    title={Massively Parallel Hyperparameter Tuning},
    author={Lisha Li and Kevin Jamieson and Afshin Rostamizadeh and Katya Gonina and Moritz Hardt and Benjamin Recht and Ameet Talwalkar},
    year={2018},
    url={https://openreview.net/forum?id=S1Y7OOlRZ},
}

@article{pytorch_lightning,
    title={PyTorch Lightning},
    author={Falcon, WA, et al.},
    journal={GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning},
    volume={3},
    year={2019}
}

@incollection{pytorch,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
    booktitle = {Advances in Neural Information Processing Systems 32},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {8024--8035},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{ray_tune,
    title={Tune: A Research Platform for Distributed Model Selection and Training},
    author={Liaw, Richard and Liang, Eric and Nishihara, Robert
        and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},
    journal={arXiv preprint arXiv:1807.05118},
    year={2018}
}

@inproceedings{pytorch_geometric,
    title={Fast Graph Representation Learning with {PyTorch Geometric}},
    author={Fey, Matthias and Lenssen, Jan E.},
    booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
    year={2019},
}
